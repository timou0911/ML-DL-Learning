{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. In the Logistic Regression slide, we used cross entropy as loss function. Please explain what is cross entropy and compare it with MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $H(p, q) = -\\sum_i p_ilog(q_i)$\n",
    "* Evaluates how much information of actual distribution($p$) is described by the predicted distribution($q$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $MSE = \\frac{1}{N}\\sum\\limits_{i=1}^N (y_i - \\hat y_i)$\n",
    "* __Problems Applied__: Cross-entropy is applied to classification, while MSE is applied to regression problems.\n",
    "* __Formula Meaning__: Cross-entropy shows how much ground truth information is described by the predicted distribution, while MSE shows the average distance between actual values and predicted values.\n",
    "* __Learning Speed__: Cross-entropy leads to quicker learning through gradient descent than MSE due to the use of sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. According to the below figure, there are 100 data, 50 data belong to Class 1 and 50 data belong to Class 2. Calculate the entropy and gini impurity of A, B and C. Tell me your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Entropy & Gini Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Left Entropy:  0.6931471805599453\n",
      "A Right Entropy:  0.6931471805599453\n",
      "A Left Gini Impurity:  0.5\n",
      "A Right Gini Impurity:  0.5\n"
     ]
    }
   ],
   "source": [
    "entropy_A_left = -((25/50)*log(25/50) + (25/50)*log(25/50))\n",
    "entropy_A_right = -((25/50)*log(25/50) + (25/50)*log(25/50))\n",
    "gini_A_left = 1 - ((25/50)**2 + (25/50)**2)\n",
    "gini_A_right = 1 - ((25/50)**2 + (25/50)**2)\n",
    "\n",
    "print(\"A Left Entropy: \", entropy_A_left)\n",
    "print(\"A Right Entropy: \", entropy_A_right)\n",
    "print(\"A Left Gini Impurity: \", gini_A_left)\n",
    "print(\"A Right Gini Impurity: \", gini_A_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B Left Entropy:  0.6365141682948128\n",
      "B Right Entropy:  -0.0\n",
      "B Left Gini Impurity:  0.4444444444444444\n",
      "B Right Gini Impurity:  0.0\n"
     ]
    }
   ],
   "source": [
    "entropy_B_left = -((50/75)*log(50/75) + (25/75)*log(25/75))\n",
    "entropy_B_right = -((25/25)*log(25/25))\n",
    "gini_B_left =  1 - ((50/75)**2 + (25/75)**2)\n",
    "gini_B_right = 1 - ((0/25)**2 + (25/25)**2)\n",
    "\n",
    "print(\"B Left Entropy: \", entropy_B_left)\n",
    "print(\"B Right Entropy: \", entropy_B_right)\n",
    "print(\"B Left Gini Impurity: \", gini_B_left)\n",
    "print(\"B Right Gini Impurity: \", gini_B_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Left Entropy:  -0.0\n",
      "C Right Entropy:  -0.0\n",
      "C Left Gini Impurity:  0.0\n",
      "C Right Gini Impurity:  0.0\n"
     ]
    }
   ],
   "source": [
    "entropy_C_left = -((50/50)*log(50/50))\n",
    "entropy_C_right = -((50/50)*log(50/50))\n",
    "gini_C_left = 1 - ((50/50)**2 + (0/50)**2)\n",
    "gini_C_right = 1 - ((0/50)**2 + (50/50)**2)\n",
    "\n",
    "print(\"C Left Entropy: \", entropy_C_left)\n",
    "print(\"C Right Entropy: \", entropy_C_right)\n",
    "print(\"C Left Gini Impurity: \", gini_C_left)\n",
    "print(\"C Right Gini Impurity: \", gini_C_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Three Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Entropy__: A measure of how disorderly a system is.\n",
    "* __Gini Impurity__: A measure of how well does a node splits the data set between the two outcomes. It stands for the probability of a randomly selected data is misclassified.\n",
    "\n",
    "With higher entropy or gini impurity, the uncertainty is larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With lower entropy and Gini impurity, Decision Tree C is the best classifier among the three trees. This is evidenced by its two leaf nodes, which perfectly classify the two classes. On the other hand, Decision Tree A is the worst classifier, as each leaf node contains a 50-50 distribution of each class, resulting in the highest entropy and Gini impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
