{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlnV5M0CCZb2j1q0/OhFXB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timou0911/ML-DL-Learning/blob/main/MNIST_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[Tutorial Following](https://www.youtube.com/watch?v=vBlO87ZAiiw)"
      ],
      "metadata": {
        "id": "xJbmWqiqXhk7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LXYXVioyIeCf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "Batch_size = 100\n",
        "LR = 0.001\n",
        "EPOCHES = 10\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeqaEBReL7LW",
        "outputId": "21fc6e28-e157-4c88-850e-2870584dfa66"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trannsform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "InC7wV_2g-P_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=trannsform)\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=trannsform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=Batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=Batch_size, shuffle=True)\n",
        "\n",
        "print(train_set)\n",
        "print(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m9B89IgEhDNx",
        "outputId": "a761f493-5947-42ee-be21-962a812d6fe7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_set.train_data.size())\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(train_set.train_data[1].numpy(), cmap='gray')\n",
        "plt.title(f'{train_set.train_labels[1]}')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(train_set.train_data[5].numpy(), cmap='gray')\n",
        "plt.title(f'{train_set.train_labels[5]}')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "-AkhoimYq4qQ",
        "outputId": "8486d7d0-f662-439e-cc36-ccf0083fd312"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    out = F.softmax(x)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "FIt7NKqQhmEk"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(DEVICE)\n",
        "print(model)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC_NdDXtiiRa",
        "outputId": "06a40def-2028-4416-de48-58417cd5dd34"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idx) % 20 == 0:\n",
        "      print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
        "\n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "      output = model(data)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      test_loss += criterion(output, target).item()\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print(f'\\nTest set: Average loss: {test_loss:.4f}, Correct: {correct}, Total: {len(test_loader.dataset)}, Accuracy: {100. * correct / len(test_loader.dataset):.3f}%\\n')\n",
        "\n",
        "def show_model_params(model):\n",
        "    for name,parameters in model.named_parameters():\n",
        "        print(name,':',parameters.size())\n",
        "show_model_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bXllXJAi_BH",
        "outputId": "d0de410b-0a96-4744-9806-a9a88c89924c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight : torch.Size([10, 1, 5, 5])\n",
            "conv1.bias : torch.Size([10])\n",
            "conv2.weight : torch.Size([20, 10, 5, 5])\n",
            "conv2.bias : torch.Size([20])\n",
            "fc1.weight : torch.Size([50, 320])\n",
            "fc1.bias : torch.Size([50])\n",
            "fc2.weight : torch.Size([10, 50])\n",
            "fc2.bias : torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHES + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBzy_DEY22w6",
        "outputId": "ece46b68-f278-43a0-c4a1-02c3e285b713"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-fa40d085c570>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000] Loss: 2.304126\n",
            "Train Epoch: 1 [2000/60000] Loss: 2.284246\n",
            "Train Epoch: 1 [4000/60000] Loss: 2.168601\n",
            "Train Epoch: 1 [6000/60000] Loss: 1.982564\n",
            "Train Epoch: 1 [8000/60000] Loss: 1.846574\n",
            "Train Epoch: 1 [10000/60000] Loss: 1.804929\n",
            "Train Epoch: 1 [12000/60000] Loss: 1.757857\n",
            "Train Epoch: 1 [14000/60000] Loss: 1.766236\n",
            "Train Epoch: 1 [16000/60000] Loss: 1.703820\n",
            "Train Epoch: 1 [18000/60000] Loss: 1.767643\n",
            "Train Epoch: 1 [20000/60000] Loss: 1.717583\n",
            "Train Epoch: 1 [22000/60000] Loss: 1.670065\n",
            "Train Epoch: 1 [24000/60000] Loss: 1.646300\n",
            "Train Epoch: 1 [26000/60000] Loss: 1.701126\n",
            "Train Epoch: 1 [28000/60000] Loss: 1.705330\n",
            "Train Epoch: 1 [30000/60000] Loss: 1.700099\n",
            "Train Epoch: 1 [32000/60000] Loss: 1.651412\n",
            "Train Epoch: 1 [34000/60000] Loss: 1.634157\n",
            "Train Epoch: 1 [36000/60000] Loss: 1.656395\n",
            "Train Epoch: 1 [38000/60000] Loss: 1.637492\n",
            "Train Epoch: 1 [40000/60000] Loss: 1.629580\n",
            "Train Epoch: 1 [42000/60000] Loss: 1.638853\n",
            "Train Epoch: 1 [44000/60000] Loss: 1.648044\n",
            "Train Epoch: 1 [46000/60000] Loss: 1.574154\n",
            "Train Epoch: 1 [48000/60000] Loss: 1.635330\n",
            "Train Epoch: 1 [50000/60000] Loss: 1.594537\n",
            "Train Epoch: 1 [52000/60000] Loss: 1.633161\n",
            "Train Epoch: 1 [54000/60000] Loss: 1.678652\n",
            "Train Epoch: 1 [56000/60000] Loss: 1.669267\n",
            "Train Epoch: 1 [58000/60000] Loss: 1.573425\n",
            "\n",
            "Test set: Average loss: 0.0153, Correct: 9293, Total: 10000, Accuracy: 92.930%\n",
            "\n",
            "Train Epoch: 2 [0/60000] Loss: 1.542664\n",
            "Train Epoch: 2 [2000/60000] Loss: 1.586225\n",
            "Train Epoch: 2 [4000/60000] Loss: 1.615333\n",
            "Train Epoch: 2 [6000/60000] Loss: 1.600319\n",
            "Train Epoch: 2 [8000/60000] Loss: 1.589067\n",
            "Train Epoch: 2 [10000/60000] Loss: 1.608961\n",
            "Train Epoch: 2 [12000/60000] Loss: 1.646556\n",
            "Train Epoch: 2 [14000/60000] Loss: 1.578683\n",
            "Train Epoch: 2 [16000/60000] Loss: 1.607755\n",
            "Train Epoch: 2 [18000/60000] Loss: 1.595282\n",
            "Train Epoch: 2 [20000/60000] Loss: 1.562345\n",
            "Train Epoch: 2 [22000/60000] Loss: 1.582167\n",
            "Train Epoch: 2 [24000/60000] Loss: 1.603091\n",
            "Train Epoch: 2 [26000/60000] Loss: 1.584772\n",
            "Train Epoch: 2 [28000/60000] Loss: 1.569735\n",
            "Train Epoch: 2 [30000/60000] Loss: 1.583066\n",
            "Train Epoch: 2 [32000/60000] Loss: 1.563975\n",
            "Train Epoch: 2 [34000/60000] Loss: 1.632160\n",
            "Train Epoch: 2 [36000/60000] Loss: 1.604985\n",
            "Train Epoch: 2 [38000/60000] Loss: 1.587203\n",
            "Train Epoch: 2 [40000/60000] Loss: 1.666351\n",
            "Train Epoch: 2 [42000/60000] Loss: 1.509606\n",
            "Train Epoch: 2 [44000/60000] Loss: 1.591246\n",
            "Train Epoch: 2 [46000/60000] Loss: 1.617685\n",
            "Train Epoch: 2 [48000/60000] Loss: 1.617251\n",
            "Train Epoch: 2 [50000/60000] Loss: 1.565348\n",
            "Train Epoch: 2 [52000/60000] Loss: 1.571328\n",
            "Train Epoch: 2 [54000/60000] Loss: 1.535336\n",
            "Train Epoch: 2 [56000/60000] Loss: 1.607288\n",
            "Train Epoch: 2 [58000/60000] Loss: 1.609617\n",
            "\n",
            "Test set: Average loss: 0.0151, Correct: 9464, Total: 10000, Accuracy: 94.640%\n",
            "\n",
            "Train Epoch: 3 [0/60000] Loss: 1.578367\n",
            "Train Epoch: 3 [2000/60000] Loss: 1.526684\n",
            "Train Epoch: 3 [4000/60000] Loss: 1.549036\n",
            "Train Epoch: 3 [6000/60000] Loss: 1.543896\n",
            "Train Epoch: 3 [8000/60000] Loss: 1.569982\n",
            "Train Epoch: 3 [10000/60000] Loss: 1.613871\n",
            "Train Epoch: 3 [12000/60000] Loss: 1.588566\n",
            "Train Epoch: 3 [14000/60000] Loss: 1.580270\n",
            "Train Epoch: 3 [16000/60000] Loss: 1.604656\n",
            "Train Epoch: 3 [18000/60000] Loss: 1.539971\n",
            "Train Epoch: 3 [20000/60000] Loss: 1.533834\n",
            "Train Epoch: 3 [22000/60000] Loss: 1.587898\n",
            "Train Epoch: 3 [24000/60000] Loss: 1.565117\n",
            "Train Epoch: 3 [26000/60000] Loss: 1.570781\n",
            "Train Epoch: 3 [28000/60000] Loss: 1.591656\n",
            "Train Epoch: 3 [30000/60000] Loss: 1.551818\n",
            "Train Epoch: 3 [32000/60000] Loss: 1.552874\n",
            "Train Epoch: 3 [34000/60000] Loss: 1.575088\n",
            "Train Epoch: 3 [36000/60000] Loss: 1.542351\n",
            "Train Epoch: 3 [38000/60000] Loss: 1.564889\n",
            "Train Epoch: 3 [40000/60000] Loss: 1.516903\n",
            "Train Epoch: 3 [42000/60000] Loss: 1.634169\n",
            "Train Epoch: 3 [44000/60000] Loss: 1.617327\n",
            "Train Epoch: 3 [46000/60000] Loss: 1.606978\n",
            "Train Epoch: 3 [48000/60000] Loss: 1.523136\n",
            "Train Epoch: 3 [50000/60000] Loss: 1.549532\n",
            "Train Epoch: 3 [52000/60000] Loss: 1.561629\n",
            "Train Epoch: 3 [54000/60000] Loss: 1.568706\n",
            "Train Epoch: 3 [56000/60000] Loss: 1.528239\n",
            "Train Epoch: 3 [58000/60000] Loss: 1.567966\n",
            "\n",
            "Test set: Average loss: 0.0150, Correct: 9576, Total: 10000, Accuracy: 95.760%\n",
            "\n",
            "Train Epoch: 4 [0/60000] Loss: 1.558898\n",
            "Train Epoch: 4 [2000/60000] Loss: 1.534406\n",
            "Train Epoch: 4 [4000/60000] Loss: 1.544948\n",
            "Train Epoch: 4 [6000/60000] Loss: 1.549357\n",
            "Train Epoch: 4 [8000/60000] Loss: 1.530986\n",
            "Train Epoch: 4 [10000/60000] Loss: 1.572218\n",
            "Train Epoch: 4 [12000/60000] Loss: 1.581026\n",
            "Train Epoch: 4 [14000/60000] Loss: 1.531461\n",
            "Train Epoch: 4 [16000/60000] Loss: 1.533256\n",
            "Train Epoch: 4 [18000/60000] Loss: 1.587503\n",
            "Train Epoch: 4 [20000/60000] Loss: 1.562148\n",
            "Train Epoch: 4 [22000/60000] Loss: 1.540807\n",
            "Train Epoch: 4 [24000/60000] Loss: 1.518339\n",
            "Train Epoch: 4 [26000/60000] Loss: 1.572321\n",
            "Train Epoch: 4 [28000/60000] Loss: 1.581512\n",
            "Train Epoch: 4 [30000/60000] Loss: 1.536096\n",
            "Train Epoch: 4 [32000/60000] Loss: 1.563894\n",
            "Train Epoch: 4 [34000/60000] Loss: 1.562934\n",
            "Train Epoch: 4 [36000/60000] Loss: 1.544193\n",
            "Train Epoch: 4 [38000/60000] Loss: 1.559996\n",
            "Train Epoch: 4 [40000/60000] Loss: 1.503744\n",
            "Train Epoch: 4 [42000/60000] Loss: 1.521157\n",
            "Train Epoch: 4 [44000/60000] Loss: 1.529206\n",
            "Train Epoch: 4 [46000/60000] Loss: 1.538943\n",
            "Train Epoch: 4 [48000/60000] Loss: 1.580923\n",
            "Train Epoch: 4 [50000/60000] Loss: 1.563815\n",
            "Train Epoch: 4 [52000/60000] Loss: 1.549079\n",
            "Train Epoch: 4 [54000/60000] Loss: 1.551420\n",
            "Train Epoch: 4 [56000/60000] Loss: 1.566583\n",
            "Train Epoch: 4 [58000/60000] Loss: 1.514749\n",
            "\n",
            "Test set: Average loss: 0.0150, Correct: 9616, Total: 10000, Accuracy: 96.160%\n",
            "\n",
            "Train Epoch: 5 [0/60000] Loss: 1.524825\n",
            "Train Epoch: 5 [2000/60000] Loss: 1.508439\n",
            "Train Epoch: 5 [4000/60000] Loss: 1.560727\n",
            "Train Epoch: 5 [6000/60000] Loss: 1.528488\n",
            "Train Epoch: 5 [8000/60000] Loss: 1.506191\n",
            "Train Epoch: 5 [10000/60000] Loss: 1.576467\n",
            "Train Epoch: 5 [12000/60000] Loss: 1.536360\n",
            "Train Epoch: 5 [14000/60000] Loss: 1.512991\n",
            "Train Epoch: 5 [16000/60000] Loss: 1.580477\n",
            "Train Epoch: 5 [18000/60000] Loss: 1.555881\n",
            "Train Epoch: 5 [20000/60000] Loss: 1.548024\n",
            "Train Epoch: 5 [22000/60000] Loss: 1.549692\n",
            "Train Epoch: 5 [24000/60000] Loss: 1.518757\n",
            "Train Epoch: 5 [26000/60000] Loss: 1.578502\n",
            "Train Epoch: 5 [28000/60000] Loss: 1.537531\n",
            "Train Epoch: 5 [30000/60000] Loss: 1.529629\n",
            "Train Epoch: 5 [32000/60000] Loss: 1.528475\n",
            "Train Epoch: 5 [34000/60000] Loss: 1.536273\n",
            "Train Epoch: 5 [36000/60000] Loss: 1.544128\n",
            "Train Epoch: 5 [38000/60000] Loss: 1.570225\n",
            "Train Epoch: 5 [40000/60000] Loss: 1.540741\n",
            "Train Epoch: 5 [42000/60000] Loss: 1.572842\n",
            "Train Epoch: 5 [44000/60000] Loss: 1.567929\n",
            "Train Epoch: 5 [46000/60000] Loss: 1.544934\n",
            "Train Epoch: 5 [48000/60000] Loss: 1.551376\n",
            "Train Epoch: 5 [50000/60000] Loss: 1.578912\n",
            "Train Epoch: 5 [52000/60000] Loss: 1.548707\n",
            "Train Epoch: 5 [54000/60000] Loss: 1.545545\n",
            "Train Epoch: 5 [56000/60000] Loss: 1.536987\n",
            "Train Epoch: 5 [58000/60000] Loss: 1.513749\n",
            "\n",
            "Test set: Average loss: 0.0150, Correct: 9650, Total: 10000, Accuracy: 96.500%\n",
            "\n",
            "Train Epoch: 6 [0/60000] Loss: 1.548097\n",
            "Train Epoch: 6 [2000/60000] Loss: 1.502343\n",
            "Train Epoch: 6 [4000/60000] Loss: 1.521869\n",
            "Train Epoch: 6 [6000/60000] Loss: 1.519658\n",
            "Train Epoch: 6 [8000/60000] Loss: 1.522266\n",
            "Train Epoch: 6 [10000/60000] Loss: 1.590048\n",
            "Train Epoch: 6 [12000/60000] Loss: 1.542029\n",
            "Train Epoch: 6 [14000/60000] Loss: 1.536302\n",
            "Train Epoch: 6 [16000/60000] Loss: 1.539847\n",
            "Train Epoch: 6 [18000/60000] Loss: 1.515030\n",
            "Train Epoch: 6 [20000/60000] Loss: 1.558632\n",
            "Train Epoch: 6 [22000/60000] Loss: 1.547485\n",
            "Train Epoch: 6 [24000/60000] Loss: 1.540803\n",
            "Train Epoch: 6 [26000/60000] Loss: 1.558790\n",
            "Train Epoch: 6 [28000/60000] Loss: 1.541614\n",
            "Train Epoch: 6 [30000/60000] Loss: 1.585349\n",
            "Train Epoch: 6 [32000/60000] Loss: 1.552547\n",
            "Train Epoch: 6 [34000/60000] Loss: 1.540569\n",
            "Train Epoch: 6 [36000/60000] Loss: 1.524150\n",
            "Train Epoch: 6 [38000/60000] Loss: 1.547431\n",
            "Train Epoch: 6 [40000/60000] Loss: 1.535598\n",
            "Train Epoch: 6 [42000/60000] Loss: 1.544523\n",
            "Train Epoch: 6 [44000/60000] Loss: 1.570456\n",
            "Train Epoch: 6 [46000/60000] Loss: 1.563801\n",
            "Train Epoch: 6 [48000/60000] Loss: 1.566300\n",
            "Train Epoch: 6 [50000/60000] Loss: 1.561713\n",
            "Train Epoch: 6 [52000/60000] Loss: 1.569356\n",
            "Train Epoch: 6 [54000/60000] Loss: 1.526606\n",
            "Train Epoch: 6 [56000/60000] Loss: 1.550798\n",
            "Train Epoch: 6 [58000/60000] Loss: 1.531942\n",
            "\n",
            "Test set: Average loss: 0.0150, Correct: 9644, Total: 10000, Accuracy: 96.440%\n",
            "\n",
            "Train Epoch: 7 [0/60000] Loss: 1.586662\n",
            "Train Epoch: 7 [2000/60000] Loss: 1.542928\n",
            "Train Epoch: 7 [4000/60000] Loss: 1.544280\n",
            "Train Epoch: 7 [6000/60000] Loss: 1.535202\n",
            "Train Epoch: 7 [8000/60000] Loss: 1.521040\n",
            "Train Epoch: 7 [10000/60000] Loss: 1.539269\n",
            "Train Epoch: 7 [12000/60000] Loss: 1.561478\n",
            "Train Epoch: 7 [14000/60000] Loss: 1.567034\n",
            "Train Epoch: 7 [16000/60000] Loss: 1.509418\n",
            "Train Epoch: 7 [18000/60000] Loss: 1.523689\n",
            "Train Epoch: 7 [20000/60000] Loss: 1.552866\n",
            "Train Epoch: 7 [22000/60000] Loss: 1.530125\n",
            "Train Epoch: 7 [24000/60000] Loss: 1.515244\n",
            "Train Epoch: 7 [26000/60000] Loss: 1.540730\n",
            "Train Epoch: 7 [28000/60000] Loss: 1.530872\n",
            "Train Epoch: 7 [30000/60000] Loss: 1.569700\n",
            "Train Epoch: 7 [32000/60000] Loss: 1.539354\n",
            "Train Epoch: 7 [34000/60000] Loss: 1.509059\n",
            "Train Epoch: 7 [36000/60000] Loss: 1.533830\n",
            "Train Epoch: 7 [38000/60000] Loss: 1.533218\n",
            "Train Epoch: 7 [40000/60000] Loss: 1.540489\n",
            "Train Epoch: 7 [42000/60000] Loss: 1.561092\n",
            "Train Epoch: 7 [44000/60000] Loss: 1.494556\n",
            "Train Epoch: 7 [46000/60000] Loss: 1.508799\n",
            "Train Epoch: 7 [48000/60000] Loss: 1.532812\n",
            "Train Epoch: 7 [50000/60000] Loss: 1.533368\n",
            "Train Epoch: 7 [52000/60000] Loss: 1.540866\n",
            "Train Epoch: 7 [54000/60000] Loss: 1.514319\n",
            "Train Epoch: 7 [56000/60000] Loss: 1.524151\n",
            "Train Epoch: 7 [58000/60000] Loss: 1.588716\n",
            "\n",
            "Test set: Average loss: 0.0149, Correct: 9683, Total: 10000, Accuracy: 96.830%\n",
            "\n",
            "Train Epoch: 8 [0/60000] Loss: 1.515633\n",
            "Train Epoch: 8 [2000/60000] Loss: 1.523975\n",
            "Train Epoch: 8 [4000/60000] Loss: 1.505141\n",
            "Train Epoch: 8 [6000/60000] Loss: 1.514389\n",
            "Train Epoch: 8 [8000/60000] Loss: 1.561005\n",
            "Train Epoch: 8 [10000/60000] Loss: 1.516300\n",
            "Train Epoch: 8 [12000/60000] Loss: 1.556269\n",
            "Train Epoch: 8 [14000/60000] Loss: 1.585023\n",
            "Train Epoch: 8 [16000/60000] Loss: 1.509589\n",
            "Train Epoch: 8 [18000/60000] Loss: 1.519755\n",
            "Train Epoch: 8 [20000/60000] Loss: 1.571142\n",
            "Train Epoch: 8 [22000/60000] Loss: 1.522391\n",
            "Train Epoch: 8 [24000/60000] Loss: 1.548716\n",
            "Train Epoch: 8 [26000/60000] Loss: 1.541919\n",
            "Train Epoch: 8 [28000/60000] Loss: 1.538485\n",
            "Train Epoch: 8 [30000/60000] Loss: 1.586016\n",
            "Train Epoch: 8 [32000/60000] Loss: 1.524892\n",
            "Train Epoch: 8 [34000/60000] Loss: 1.551789\n",
            "Train Epoch: 8 [36000/60000] Loss: 1.515172\n",
            "Train Epoch: 8 [38000/60000] Loss: 1.562036\n",
            "Train Epoch: 8 [40000/60000] Loss: 1.551771\n",
            "Train Epoch: 8 [42000/60000] Loss: 1.507981\n",
            "Train Epoch: 8 [44000/60000] Loss: 1.552072\n",
            "Train Epoch: 8 [46000/60000] Loss: 1.518669\n",
            "Train Epoch: 8 [48000/60000] Loss: 1.551255\n",
            "Train Epoch: 8 [50000/60000] Loss: 1.545569\n",
            "Train Epoch: 8 [52000/60000] Loss: 1.520314\n",
            "Train Epoch: 8 [54000/60000] Loss: 1.561237\n",
            "Train Epoch: 8 [56000/60000] Loss: 1.546597\n",
            "Train Epoch: 8 [58000/60000] Loss: 1.508640\n",
            "\n",
            "Test set: Average loss: 0.0149, Correct: 9699, Total: 10000, Accuracy: 96.990%\n",
            "\n",
            "Train Epoch: 9 [0/60000] Loss: 1.530732\n",
            "Train Epoch: 9 [2000/60000] Loss: 1.567027\n",
            "Train Epoch: 9 [4000/60000] Loss: 1.566571\n",
            "Train Epoch: 9 [6000/60000] Loss: 1.501355\n",
            "Train Epoch: 9 [8000/60000] Loss: 1.505159\n",
            "Train Epoch: 9 [10000/60000] Loss: 1.555141\n",
            "Train Epoch: 9 [12000/60000] Loss: 1.493126\n",
            "Train Epoch: 9 [14000/60000] Loss: 1.521580\n",
            "Train Epoch: 9 [16000/60000] Loss: 1.561702\n",
            "Train Epoch: 9 [18000/60000] Loss: 1.537299\n",
            "Train Epoch: 9 [20000/60000] Loss: 1.521015\n",
            "Train Epoch: 9 [22000/60000] Loss: 1.522243\n",
            "Train Epoch: 9 [24000/60000] Loss: 1.502392\n",
            "Train Epoch: 9 [26000/60000] Loss: 1.515714\n",
            "Train Epoch: 9 [28000/60000] Loss: 1.554380\n",
            "Train Epoch: 9 [30000/60000] Loss: 1.528098\n",
            "Train Epoch: 9 [32000/60000] Loss: 1.530538\n",
            "Train Epoch: 9 [34000/60000] Loss: 1.543012\n",
            "Train Epoch: 9 [36000/60000] Loss: 1.500979\n",
            "Train Epoch: 9 [38000/60000] Loss: 1.483028\n",
            "Train Epoch: 9 [40000/60000] Loss: 1.552210\n",
            "Train Epoch: 9 [42000/60000] Loss: 1.526363\n",
            "Train Epoch: 9 [44000/60000] Loss: 1.568001\n",
            "Train Epoch: 9 [46000/60000] Loss: 1.530741\n",
            "Train Epoch: 9 [48000/60000] Loss: 1.540105\n",
            "Train Epoch: 9 [50000/60000] Loss: 1.515192\n",
            "Train Epoch: 9 [52000/60000] Loss: 1.531610\n",
            "Train Epoch: 9 [54000/60000] Loss: 1.518776\n",
            "Train Epoch: 9 [56000/60000] Loss: 1.520141\n",
            "Train Epoch: 9 [58000/60000] Loss: 1.536404\n",
            "\n",
            "Test set: Average loss: 0.0149, Correct: 9686, Total: 10000, Accuracy: 96.860%\n",
            "\n",
            "Train Epoch: 10 [0/60000] Loss: 1.529942\n",
            "Train Epoch: 10 [2000/60000] Loss: 1.519709\n",
            "Train Epoch: 10 [4000/60000] Loss: 1.569071\n",
            "Train Epoch: 10 [6000/60000] Loss: 1.554561\n",
            "Train Epoch: 10 [8000/60000] Loss: 1.518948\n",
            "Train Epoch: 10 [10000/60000] Loss: 1.599402\n",
            "Train Epoch: 10 [12000/60000] Loss: 1.552361\n",
            "Train Epoch: 10 [14000/60000] Loss: 1.540003\n",
            "Train Epoch: 10 [16000/60000] Loss: 1.554042\n",
            "Train Epoch: 10 [18000/60000] Loss: 1.560390\n",
            "Train Epoch: 10 [20000/60000] Loss: 1.551781\n",
            "Train Epoch: 10 [22000/60000] Loss: 1.558295\n",
            "Train Epoch: 10 [24000/60000] Loss: 1.519836\n",
            "Train Epoch: 10 [26000/60000] Loss: 1.516986\n",
            "Train Epoch: 10 [28000/60000] Loss: 1.497659\n",
            "Train Epoch: 10 [30000/60000] Loss: 1.549817\n",
            "Train Epoch: 10 [32000/60000] Loss: 1.565807\n",
            "Train Epoch: 10 [34000/60000] Loss: 1.542666\n",
            "Train Epoch: 10 [36000/60000] Loss: 1.496470\n",
            "Train Epoch: 10 [38000/60000] Loss: 1.502134\n",
            "Train Epoch: 10 [40000/60000] Loss: 1.504264\n",
            "Train Epoch: 10 [42000/60000] Loss: 1.508335\n",
            "Train Epoch: 10 [44000/60000] Loss: 1.528323\n",
            "Train Epoch: 10 [46000/60000] Loss: 1.522970\n",
            "Train Epoch: 10 [48000/60000] Loss: 1.534596\n",
            "Train Epoch: 10 [50000/60000] Loss: 1.519859\n",
            "Train Epoch: 10 [52000/60000] Loss: 1.524883\n",
            "Train Epoch: 10 [54000/60000] Loss: 1.490180\n",
            "Train Epoch: 10 [56000/60000] Loss: 1.484591\n",
            "Train Epoch: 10 [58000/60000] Loss: 1.540868\n",
            "\n",
            "Test set: Average loss: 0.0149, Correct: 9742, Total: 10000, Accuracy: 97.420%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "data, target = test_set[0]\n",
        "data = data.unsqueeze(0).to(DEVICE)\n",
        "output = model(data)\n",
        "prediction = output.argmax(dim=1, keepdim=True).item()\n",
        "print(f'Prediction: {prediction}')\n",
        "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "gUw9JAoz3A3I",
        "outputId": "b465798e-1345-4087-a01b-3fe9c448b25e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-fa40d085c570>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.softmax(x)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}