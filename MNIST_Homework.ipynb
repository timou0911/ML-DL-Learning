{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObVur21SXTy+IEtyiF6LKJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timou0911/ML-DL-Learning/blob/main/MNIST_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[Tutorial Following](https://www.youtube.com/watch?v=vBlO87ZAiiw)"
      ],
      "metadata": {
        "id": "xJbmWqiqXhk7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LXYXVioyIeCf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "Batch_size = 100\n",
        "LR = 0.001\n",
        "EPOCHES = 10\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeqaEBReL7LW",
        "outputId": "6c01ef6f-07fe-4b44-ac81-529f257d3ed3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trannsform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "InC7wV_2g-P_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=trannsform)\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=trannsform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=Batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=Batch_size, shuffle=True)\n",
        "\n",
        "print(train_set)\n",
        "print(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m9B89IgEhDNx",
        "outputId": "0cd1c71b-5c9c-469b-ecae-9b9caba9f74a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_set.train_data.size())\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(train_set.train_data[1].numpy(), cmap='gray')\n",
        "plt.title(f'{train_set.train_labels[1]}')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(train_set.train_data[5].numpy(), cmap='gray')\n",
        "plt.title(f'{train_set.train_labels[5]}')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "-AkhoimYq4qQ",
        "outputId": "6e41df2e-2aa9-404d-8b05-9440fdf7500c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:66: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    out = F.softmax(x)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "FIt7NKqQhmEk"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(DEVICE)\n",
        "print(model)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC_NdDXtiiRa",
        "outputId": "27da7f96-1c16-427f-e556-bbead73ea626"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idx) % 20 == 0:\n",
        "      print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
        "\n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "      output = model(data)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      test_loss += criterion(output, target).item()\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print(f'\\nTest set: Average loss: {test_loss:.4f}, Correct: {correct}, Total: {len(test_loader.dataset)}, Accuracy: {100. * correct / len(test_loader.dataset):.3f}%\\n')\n",
        "\n",
        "def show_model_params(model):\n",
        "    for name,parameters in model.named_parameters():\n",
        "        print(name,':',parameters.size())\n",
        "show_model_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bXllXJAi_BH",
        "outputId": "67d0e93d-58dc-440b-eb6b-fa8c8d7e5f50"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight : torch.Size([10, 1, 5, 5])\n",
            "conv1.bias : torch.Size([10])\n",
            "conv2.weight : torch.Size([20, 10, 5, 5])\n",
            "conv2.bias : torch.Size([20])\n",
            "fc1.weight : torch.Size([50, 320])\n",
            "fc1.bias : torch.Size([50])\n",
            "fc2.weight : torch.Size([10, 50])\n",
            "fc2.bias : torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHES+1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBzy_DEY22w6",
        "outputId": "c96b777a-7498-4f6d-e561-4ca52e35161f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-4cf3c264966a>:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000] Loss: 2.303228\n",
            "Train Epoch: 1 [2000/60000] Loss: 2.286807\n",
            "Train Epoch: 1 [4000/60000] Loss: 2.169188\n",
            "Train Epoch: 1 [6000/60000] Loss: 2.008614\n",
            "Train Epoch: 1 [8000/60000] Loss: 1.924168\n",
            "Train Epoch: 1 [10000/60000] Loss: 1.799289\n",
            "Train Epoch: 1 [12000/60000] Loss: 1.847059\n",
            "Train Epoch: 1 [14000/60000] Loss: 1.863413\n",
            "Train Epoch: 1 [16000/60000] Loss: 1.736198\n",
            "Train Epoch: 1 [18000/60000] Loss: 1.773885\n",
            "Train Epoch: 1 [20000/60000] Loss: 1.786215\n",
            "Train Epoch: 1 [22000/60000] Loss: 1.682561\n",
            "Train Epoch: 1 [24000/60000] Loss: 1.793355\n",
            "Train Epoch: 1 [26000/60000] Loss: 1.662872\n",
            "Train Epoch: 1 [28000/60000] Loss: 1.674138\n",
            "Train Epoch: 1 [30000/60000] Loss: 1.643745\n",
            "Train Epoch: 1 [32000/60000] Loss: 1.629686\n",
            "Train Epoch: 1 [34000/60000] Loss: 1.630488\n",
            "Train Epoch: 1 [36000/60000] Loss: 1.721272\n",
            "Train Epoch: 1 [38000/60000] Loss: 1.660743\n",
            "Train Epoch: 1 [40000/60000] Loss: 1.688154\n",
            "Train Epoch: 1 [42000/60000] Loss: 1.621280\n",
            "Train Epoch: 1 [44000/60000] Loss: 1.642783\n",
            "Train Epoch: 1 [46000/60000] Loss: 1.643461\n",
            "Train Epoch: 1 [48000/60000] Loss: 1.645027\n",
            "Train Epoch: 1 [50000/60000] Loss: 1.664882\n",
            "Train Epoch: 1 [52000/60000] Loss: 1.639341\n",
            "Train Epoch: 1 [54000/60000] Loss: 1.644122\n",
            "Train Epoch: 1 [56000/60000] Loss: 1.590138\n",
            "Train Epoch: 1 [58000/60000] Loss: 1.642473\n",
            "\n",
            "Test set: Average loss: 0.0153, Correct: 9342, Total: 10000, Accuracy: 93.420%\n",
            "\n",
            "Train Epoch: 2 [0/60000] Loss: 1.615620\n",
            "Train Epoch: 2 [2000/60000] Loss: 1.617867\n",
            "Train Epoch: 2 [4000/60000] Loss: 1.594047\n",
            "Train Epoch: 2 [6000/60000] Loss: 1.565771\n",
            "Train Epoch: 2 [8000/60000] Loss: 1.597661\n",
            "Train Epoch: 2 [10000/60000] Loss: 1.579241\n",
            "Train Epoch: 2 [12000/60000] Loss: 1.599436\n",
            "Train Epoch: 2 [14000/60000] Loss: 1.577176\n",
            "Train Epoch: 2 [16000/60000] Loss: 1.586640\n",
            "Train Epoch: 2 [18000/60000] Loss: 1.610085\n",
            "Train Epoch: 2 [20000/60000] Loss: 1.595507\n",
            "Train Epoch: 2 [22000/60000] Loss: 1.555137\n",
            "Train Epoch: 2 [24000/60000] Loss: 1.633150\n",
            "Train Epoch: 2 [26000/60000] Loss: 1.562465\n",
            "Train Epoch: 2 [28000/60000] Loss: 1.634942\n",
            "Train Epoch: 2 [30000/60000] Loss: 1.575973\n",
            "Train Epoch: 2 [32000/60000] Loss: 1.610881\n",
            "Train Epoch: 2 [34000/60000] Loss: 1.559027\n",
            "Train Epoch: 2 [36000/60000] Loss: 1.586555\n",
            "Train Epoch: 2 [38000/60000] Loss: 1.542184\n",
            "Train Epoch: 2 [40000/60000] Loss: 1.588438\n",
            "Train Epoch: 2 [42000/60000] Loss: 1.596104\n",
            "Train Epoch: 2 [44000/60000] Loss: 1.606765\n",
            "Train Epoch: 2 [46000/60000] Loss: 1.558147\n",
            "Train Epoch: 2 [48000/60000] Loss: 1.577762\n",
            "Train Epoch: 2 [50000/60000] Loss: 1.610927\n",
            "Train Epoch: 2 [52000/60000] Loss: 1.581718\n",
            "Train Epoch: 2 [54000/60000] Loss: 1.564508\n",
            "Train Epoch: 2 [56000/60000] Loss: 1.582212\n",
            "Train Epoch: 2 [58000/60000] Loss: 1.534989\n",
            "\n",
            "Test set: Average loss: 0.0150, Correct: 9580, Total: 10000, Accuracy: 95.800%\n",
            "\n",
            "Train Epoch: 3 [0/60000] Loss: 1.599520\n",
            "Train Epoch: 3 [2000/60000] Loss: 1.563789\n",
            "Train Epoch: 3 [4000/60000] Loss: 1.567013\n",
            "Train Epoch: 3 [6000/60000] Loss: 1.522631\n",
            "Train Epoch: 3 [8000/60000] Loss: 1.578520\n",
            "Train Epoch: 3 [10000/60000] Loss: 1.562285\n",
            "Train Epoch: 3 [12000/60000] Loss: 1.561091\n",
            "Train Epoch: 3 [14000/60000] Loss: 1.566007\n",
            "Train Epoch: 3 [16000/60000] Loss: 1.574068\n",
            "Train Epoch: 3 [18000/60000] Loss: 1.559369\n",
            "Train Epoch: 3 [20000/60000] Loss: 1.542867\n",
            "Train Epoch: 3 [22000/60000] Loss: 1.559339\n",
            "Train Epoch: 3 [24000/60000] Loss: 1.555102\n",
            "Train Epoch: 3 [26000/60000] Loss: 1.526126\n",
            "Train Epoch: 3 [28000/60000] Loss: 1.560812\n",
            "Train Epoch: 3 [30000/60000] Loss: 1.541459\n",
            "Train Epoch: 3 [32000/60000] Loss: 1.525619\n",
            "Train Epoch: 3 [34000/60000] Loss: 1.540395\n",
            "Train Epoch: 3 [36000/60000] Loss: 1.536563\n",
            "Train Epoch: 3 [38000/60000] Loss: 1.580009\n",
            "Train Epoch: 3 [40000/60000] Loss: 1.553701\n",
            "Train Epoch: 3 [42000/60000] Loss: 1.559619\n",
            "Train Epoch: 3 [44000/60000] Loss: 1.608143\n",
            "Train Epoch: 3 [46000/60000] Loss: 1.532260\n",
            "Train Epoch: 3 [48000/60000] Loss: 1.527718\n",
            "Train Epoch: 3 [50000/60000] Loss: 1.641321\n",
            "Train Epoch: 3 [52000/60000] Loss: 1.500929\n",
            "Train Epoch: 3 [54000/60000] Loss: 1.512577\n",
            "Train Epoch: 3 [56000/60000] Loss: 1.569772\n",
            "Train Epoch: 3 [58000/60000] Loss: 1.521146\n",
            "\n",
            "Test set: Average loss: 0.0150, Correct: 9635, Total: 10000, Accuracy: 96.350%\n",
            "\n",
            "Train Epoch: 4 [0/60000] Loss: 1.537072\n",
            "Train Epoch: 4 [2000/60000] Loss: 1.539607\n",
            "Train Epoch: 4 [4000/60000] Loss: 1.538256\n",
            "Train Epoch: 4 [6000/60000] Loss: 1.511646\n",
            "Train Epoch: 4 [8000/60000] Loss: 1.519799\n",
            "Train Epoch: 4 [10000/60000] Loss: 1.554905\n",
            "Train Epoch: 4 [12000/60000] Loss: 1.519795\n",
            "Train Epoch: 4 [14000/60000] Loss: 1.543862\n",
            "Train Epoch: 4 [16000/60000] Loss: 1.520859\n",
            "Train Epoch: 4 [18000/60000] Loss: 1.537379\n",
            "Train Epoch: 4 [20000/60000] Loss: 1.540413\n",
            "Train Epoch: 4 [22000/60000] Loss: 1.539584\n",
            "Train Epoch: 4 [24000/60000] Loss: 1.556289\n",
            "Train Epoch: 4 [26000/60000] Loss: 1.510718\n",
            "Train Epoch: 4 [28000/60000] Loss: 1.582225\n",
            "Train Epoch: 4 [30000/60000] Loss: 1.543690\n",
            "Train Epoch: 4 [32000/60000] Loss: 1.536009\n",
            "Train Epoch: 4 [34000/60000] Loss: 1.547487\n",
            "Train Epoch: 4 [36000/60000] Loss: 1.536186\n",
            "Train Epoch: 4 [38000/60000] Loss: 1.553325\n",
            "Train Epoch: 4 [40000/60000] Loss: 1.521083\n",
            "Train Epoch: 4 [42000/60000] Loss: 1.535675\n",
            "Train Epoch: 4 [44000/60000] Loss: 1.602851\n",
            "Train Epoch: 4 [46000/60000] Loss: 1.567017\n",
            "Train Epoch: 4 [48000/60000] Loss: 1.570295\n",
            "Train Epoch: 4 [50000/60000] Loss: 1.536515\n",
            "Train Epoch: 4 [52000/60000] Loss: 1.542466\n",
            "Train Epoch: 4 [54000/60000] Loss: 1.623043\n",
            "Train Epoch: 4 [56000/60000] Loss: 1.562723\n",
            "Train Epoch: 4 [58000/60000] Loss: 1.523258\n",
            "\n",
            "Test set: Average loss: 0.0149, Correct: 9676, Total: 10000, Accuracy: 96.760%\n",
            "\n",
            "Train Epoch: 5 [0/60000] Loss: 1.566920\n",
            "Train Epoch: 5 [2000/60000] Loss: 1.577524\n",
            "Train Epoch: 5 [4000/60000] Loss: 1.518617\n",
            "Train Epoch: 5 [6000/60000] Loss: 1.532143\n",
            "Train Epoch: 5 [8000/60000] Loss: 1.539086\n",
            "Train Epoch: 5 [10000/60000] Loss: 1.533039\n",
            "Train Epoch: 5 [12000/60000] Loss: 1.531026\n",
            "Train Epoch: 5 [14000/60000] Loss: 1.539250\n",
            "Train Epoch: 5 [16000/60000] Loss: 1.522290\n",
            "Train Epoch: 5 [18000/60000] Loss: 1.534357\n",
            "Train Epoch: 5 [20000/60000] Loss: 1.560544\n",
            "Train Epoch: 5 [22000/60000] Loss: 1.552395\n",
            "Train Epoch: 5 [24000/60000] Loss: 1.533726\n",
            "Train Epoch: 5 [26000/60000] Loss: 1.500362\n",
            "Train Epoch: 5 [28000/60000] Loss: 1.577376\n",
            "Train Epoch: 5 [30000/60000] Loss: 1.553087\n",
            "Train Epoch: 5 [32000/60000] Loss: 1.539851\n",
            "Train Epoch: 5 [34000/60000] Loss: 1.515027\n",
            "Train Epoch: 5 [36000/60000] Loss: 1.555981\n",
            "Train Epoch: 5 [38000/60000] Loss: 1.574491\n",
            "Train Epoch: 5 [40000/60000] Loss: 1.545419\n",
            "Train Epoch: 5 [42000/60000] Loss: 1.556308\n",
            "Train Epoch: 5 [44000/60000] Loss: 1.500689\n",
            "Train Epoch: 5 [46000/60000] Loss: 1.543265\n",
            "Train Epoch: 5 [48000/60000] Loss: 1.520700\n",
            "Train Epoch: 5 [50000/60000] Loss: 1.594470\n",
            "Train Epoch: 5 [52000/60000] Loss: 1.541511\n",
            "Train Epoch: 5 [54000/60000] Loss: 1.539822\n",
            "Train Epoch: 5 [56000/60000] Loss: 1.518536\n",
            "Train Epoch: 5 [58000/60000] Loss: 1.548911\n",
            "\n",
            "Test set: Average loss: 0.0149, Correct: 9723, Total: 10000, Accuracy: 97.230%\n",
            "\n",
            "Train Epoch: 6 [0/60000] Loss: 1.537398\n",
            "Train Epoch: 6 [2000/60000] Loss: 1.550074\n",
            "Train Epoch: 6 [4000/60000] Loss: 1.560470\n",
            "Train Epoch: 6 [6000/60000] Loss: 1.528814\n",
            "Train Epoch: 6 [8000/60000] Loss: 1.538058\n",
            "Train Epoch: 6 [10000/60000] Loss: 1.502517\n",
            "Train Epoch: 6 [12000/60000] Loss: 1.526063\n",
            "Train Epoch: 6 [14000/60000] Loss: 1.554072\n",
            "Train Epoch: 6 [16000/60000] Loss: 1.571074\n",
            "Train Epoch: 6 [18000/60000] Loss: 1.518981\n",
            "Train Epoch: 6 [20000/60000] Loss: 1.542020\n",
            "Train Epoch: 6 [22000/60000] Loss: 1.521616\n",
            "Train Epoch: 6 [24000/60000] Loss: 1.551006\n",
            "Train Epoch: 6 [26000/60000] Loss: 1.538641\n",
            "Train Epoch: 6 [28000/60000] Loss: 1.512745\n",
            "Train Epoch: 6 [30000/60000] Loss: 1.556324\n",
            "Train Epoch: 6 [32000/60000] Loss: 1.534601\n",
            "Train Epoch: 6 [34000/60000] Loss: 1.560955\n",
            "Train Epoch: 6 [36000/60000] Loss: 1.507370\n",
            "Train Epoch: 6 [38000/60000] Loss: 1.562776\n",
            "Train Epoch: 6 [40000/60000] Loss: 1.497692\n",
            "Train Epoch: 6 [42000/60000] Loss: 1.513191\n",
            "Train Epoch: 6 [44000/60000] Loss: 1.530674\n",
            "Train Epoch: 6 [46000/60000] Loss: 1.502647\n",
            "Train Epoch: 6 [48000/60000] Loss: 1.509123\n",
            "Train Epoch: 6 [50000/60000] Loss: 1.491183\n",
            "Train Epoch: 6 [52000/60000] Loss: 1.492605\n",
            "Train Epoch: 6 [54000/60000] Loss: 1.493083\n",
            "Train Epoch: 6 [56000/60000] Loss: 1.561350\n",
            "Train Epoch: 6 [58000/60000] Loss: 1.529793\n",
            "\n",
            "Test set: Average loss: 0.0149, Correct: 9749, Total: 10000, Accuracy: 97.490%\n",
            "\n",
            "Train Epoch: 7 [0/60000] Loss: 1.534789\n",
            "Train Epoch: 7 [2000/60000] Loss: 1.506693\n",
            "Train Epoch: 7 [4000/60000] Loss: 1.500570\n",
            "Train Epoch: 7 [6000/60000] Loss: 1.533796\n",
            "Train Epoch: 7 [8000/60000] Loss: 1.522111\n",
            "Train Epoch: 7 [10000/60000] Loss: 1.516897\n",
            "Train Epoch: 7 [12000/60000] Loss: 1.523640\n",
            "Train Epoch: 7 [14000/60000] Loss: 1.511052\n",
            "Train Epoch: 7 [16000/60000] Loss: 1.519149\n",
            "Train Epoch: 7 [18000/60000] Loss: 1.520186\n",
            "Train Epoch: 7 [20000/60000] Loss: 1.493996\n",
            "Train Epoch: 7 [22000/60000] Loss: 1.512781\n",
            "Train Epoch: 7 [24000/60000] Loss: 1.542192\n",
            "Train Epoch: 7 [26000/60000] Loss: 1.519965\n",
            "Train Epoch: 7 [28000/60000] Loss: 1.535277\n",
            "Train Epoch: 7 [30000/60000] Loss: 1.525813\n",
            "Train Epoch: 7 [32000/60000] Loss: 1.541148\n",
            "Train Epoch: 7 [34000/60000] Loss: 1.536662\n",
            "Train Epoch: 7 [36000/60000] Loss: 1.514956\n",
            "Train Epoch: 7 [38000/60000] Loss: 1.508230\n",
            "Train Epoch: 7 [40000/60000] Loss: 1.504446\n",
            "Train Epoch: 7 [42000/60000] Loss: 1.528659\n",
            "Train Epoch: 7 [44000/60000] Loss: 1.528525\n",
            "Train Epoch: 7 [46000/60000] Loss: 1.506262\n",
            "Train Epoch: 7 [48000/60000] Loss: 1.498853\n",
            "Train Epoch: 7 [50000/60000] Loss: 1.531846\n",
            "Train Epoch: 7 [52000/60000] Loss: 1.493867\n",
            "Train Epoch: 7 [54000/60000] Loss: 1.532132\n",
            "Train Epoch: 7 [56000/60000] Loss: 1.513737\n",
            "Train Epoch: 7 [58000/60000] Loss: 1.504779\n",
            "\n",
            "Test set: Average loss: 0.0149, Correct: 9749, Total: 10000, Accuracy: 97.490%\n",
            "\n",
            "Train Epoch: 8 [0/60000] Loss: 1.523561\n",
            "Train Epoch: 8 [2000/60000] Loss: 1.505358\n",
            "Train Epoch: 8 [4000/60000] Loss: 1.522293\n",
            "Train Epoch: 8 [6000/60000] Loss: 1.518992\n",
            "Train Epoch: 8 [8000/60000] Loss: 1.482578\n",
            "Train Epoch: 8 [10000/60000] Loss: 1.519968\n",
            "Train Epoch: 8 [12000/60000] Loss: 1.495221\n",
            "Train Epoch: 8 [14000/60000] Loss: 1.528975\n",
            "Train Epoch: 8 [16000/60000] Loss: 1.546896\n",
            "Train Epoch: 8 [18000/60000] Loss: 1.498727\n",
            "Train Epoch: 8 [20000/60000] Loss: 1.505383\n",
            "Train Epoch: 8 [22000/60000] Loss: 1.537647\n",
            "Train Epoch: 8 [24000/60000] Loss: 1.513528\n",
            "Train Epoch: 8 [26000/60000] Loss: 1.495431\n",
            "Train Epoch: 8 [28000/60000] Loss: 1.511487\n",
            "Train Epoch: 8 [30000/60000] Loss: 1.503796\n",
            "Train Epoch: 8 [32000/60000] Loss: 1.513385\n",
            "Train Epoch: 8 [34000/60000] Loss: 1.562044\n",
            "Train Epoch: 8 [36000/60000] Loss: 1.534165\n",
            "Train Epoch: 8 [38000/60000] Loss: 1.523253\n",
            "Train Epoch: 8 [40000/60000] Loss: 1.576175\n",
            "Train Epoch: 8 [42000/60000] Loss: 1.538087\n",
            "Train Epoch: 8 [44000/60000] Loss: 1.548757\n",
            "Train Epoch: 8 [46000/60000] Loss: 1.504422\n",
            "Train Epoch: 8 [48000/60000] Loss: 1.525676\n",
            "Train Epoch: 8 [50000/60000] Loss: 1.518308\n",
            "Train Epoch: 8 [52000/60000] Loss: 1.507630\n",
            "Train Epoch: 8 [54000/60000] Loss: 1.519380\n",
            "Train Epoch: 8 [56000/60000] Loss: 1.532231\n",
            "Train Epoch: 8 [58000/60000] Loss: 1.507221\n",
            "\n",
            "Test set: Average loss: 0.0148, Correct: 9768, Total: 10000, Accuracy: 97.680%\n",
            "\n",
            "Train Epoch: 9 [0/60000] Loss: 1.510193\n",
            "Train Epoch: 9 [2000/60000] Loss: 1.528107\n",
            "Train Epoch: 9 [4000/60000] Loss: 1.483015\n",
            "Train Epoch: 9 [6000/60000] Loss: 1.504701\n",
            "Train Epoch: 9 [8000/60000] Loss: 1.501085\n",
            "Train Epoch: 9 [10000/60000] Loss: 1.500133\n",
            "Train Epoch: 9 [12000/60000] Loss: 1.509821\n",
            "Train Epoch: 9 [14000/60000] Loss: 1.521684\n",
            "Train Epoch: 9 [16000/60000] Loss: 1.500212\n",
            "Train Epoch: 9 [18000/60000] Loss: 1.492659\n",
            "Train Epoch: 9 [20000/60000] Loss: 1.487490\n",
            "Train Epoch: 9 [22000/60000] Loss: 1.550693\n",
            "Train Epoch: 9 [24000/60000] Loss: 1.466584\n",
            "Train Epoch: 9 [26000/60000] Loss: 1.506606\n",
            "Train Epoch: 9 [28000/60000] Loss: 1.493499\n",
            "Train Epoch: 9 [30000/60000] Loss: 1.534374\n",
            "Train Epoch: 9 [32000/60000] Loss: 1.502942\n",
            "Train Epoch: 9 [34000/60000] Loss: 1.563449\n",
            "Train Epoch: 9 [36000/60000] Loss: 1.521791\n",
            "Train Epoch: 9 [38000/60000] Loss: 1.535248\n",
            "Train Epoch: 9 [40000/60000] Loss: 1.580617\n",
            "Train Epoch: 9 [42000/60000] Loss: 1.498765\n",
            "Train Epoch: 9 [44000/60000] Loss: 1.517547\n",
            "Train Epoch: 9 [46000/60000] Loss: 1.498179\n",
            "Train Epoch: 9 [48000/60000] Loss: 1.547973\n",
            "Train Epoch: 9 [50000/60000] Loss: 1.514888\n",
            "Train Epoch: 9 [52000/60000] Loss: 1.505497\n",
            "Train Epoch: 9 [54000/60000] Loss: 1.536556\n",
            "Train Epoch: 9 [56000/60000] Loss: 1.540356\n",
            "Train Epoch: 9 [58000/60000] Loss: 1.505893\n",
            "\n",
            "Test set: Average loss: 0.0148, Correct: 9793, Total: 10000, Accuracy: 97.930%\n",
            "\n",
            "Train Epoch: 10 [0/60000] Loss: 1.502469\n",
            "Train Epoch: 10 [2000/60000] Loss: 1.515671\n",
            "Train Epoch: 10 [4000/60000] Loss: 1.509733\n",
            "Train Epoch: 10 [6000/60000] Loss: 1.509210\n",
            "Train Epoch: 10 [8000/60000] Loss: 1.523326\n",
            "Train Epoch: 10 [10000/60000] Loss: 1.523470\n",
            "Train Epoch: 10 [12000/60000] Loss: 1.505091\n",
            "Train Epoch: 10 [14000/60000] Loss: 1.503356\n",
            "Train Epoch: 10 [16000/60000] Loss: 1.495112\n",
            "Train Epoch: 10 [18000/60000] Loss: 1.512360\n",
            "Train Epoch: 10 [20000/60000] Loss: 1.499127\n",
            "Train Epoch: 10 [22000/60000] Loss: 1.529468\n",
            "Train Epoch: 10 [24000/60000] Loss: 1.493744\n",
            "Train Epoch: 10 [26000/60000] Loss: 1.478019\n",
            "Train Epoch: 10 [28000/60000] Loss: 1.525445\n",
            "Train Epoch: 10 [30000/60000] Loss: 1.512263\n",
            "Train Epoch: 10 [32000/60000] Loss: 1.483925\n",
            "Train Epoch: 10 [34000/60000] Loss: 1.529596\n",
            "Train Epoch: 10 [36000/60000] Loss: 1.513309\n",
            "Train Epoch: 10 [38000/60000] Loss: 1.488117\n",
            "Train Epoch: 10 [40000/60000] Loss: 1.520718\n",
            "Train Epoch: 10 [42000/60000] Loss: 1.512504\n",
            "Train Epoch: 10 [44000/60000] Loss: 1.508597\n",
            "Train Epoch: 10 [46000/60000] Loss: 1.525891\n",
            "Train Epoch: 10 [48000/60000] Loss: 1.513537\n",
            "Train Epoch: 10 [50000/60000] Loss: 1.484339\n",
            "Train Epoch: 10 [52000/60000] Loss: 1.526631\n",
            "Train Epoch: 10 [54000/60000] Loss: 1.480623\n",
            "Train Epoch: 10 [56000/60000] Loss: 1.495809\n",
            "Train Epoch: 10 [58000/60000] Loss: 1.522416\n",
            "\n",
            "Test set: Average loss: 0.0148, Correct: 9813, Total: 10000, Accuracy: 98.130%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "data, target = test_set[0]\n",
        "data = data.unsqueeze(0).to(DEVICE)\n",
        "output = model(data)\n",
        "prediction = output.argmax(dim=1, keepdim=True).item()\n",
        "print(f'Prediction: {prediction}')\n",
        "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "gUw9JAoz3A3I",
        "outputId": "72e1dd05-53d7-4a6e-8cd6-451b91667432"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-4cf3c264966a>:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}